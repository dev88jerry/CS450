{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqyFLPDojx9r",
        "outputId": "fa0f816a-7915-4b6f-f0fd-2285fa98251c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=ec2a7493e9b1fc8e3965c242f8a2501aaf0777f65e6370adeaa8a2314b7758b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "print(spark)\n",
        "# Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW0szn8Rj7RT",
        "outputId": "2ddd9a58-f2a7-47d5-88bf-9231cbf61873"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7a53ac174ee0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qoFyFiuVWl2",
        "outputId": "58e05351-eb54-4470-fd0d-da6dda34278f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYsYwb-lVzve",
        "outputId": "1fd5385c-6d4b-41af-9ccb-49d71bc8c2f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark reduceByKey usage with example**"
      ],
      "metadata": {
        "id": "saaCVCjOWEic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark *reduceByKey()* transformation is used to merge the values of each key using an associative reduce function on PySpark RDD. It is a wider transformation as it shuffles data across multiple partitions and It operates on pair RDD (key/value pair).\n",
        "\n",
        "When *reduceByKey()* performs, the output will be partitioned by either numPartitions or the default parallelism level. The Default partitioner is hash-partition.\n",
        "\n",
        "First, let’s create an RDD from the list.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JYRTmTnWWXL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('Project', 1),\n",
        "('Gutenberg’s', 1),\n",
        "('Alice’s', 1),\n",
        "('Adventures', 1),\n",
        "('in', 1),\n",
        "('Wonderland', 1),\n",
        "('Project', 1),\n",
        "('Gutenberg’s', 1),\n",
        "('Adventures', 1),\n",
        "('in', 1),\n",
        "('Wonderland', 1),\n",
        "('Project', 1),\n",
        "('Gutenberg’s', 1)]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "i5o6dzG8XYKq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduceByKey() Example**\n",
        "\n",
        "In our example, we use PySpark reduceByKey() to reduces the word string by applying the sum function on value. The result of our RDD contains unique words and their count.\n"
      ],
      "metadata": {
        "id": "zsi7PALsXoLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2=rdd.reduceByKey(lambda a,b: a+b)\n",
        "for element in rdd2.collect():\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd9Ax55LXrQW",
        "outputId": "8c24f4dd-f4af-41cd-de00-9148e1044fb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Project', 3)\n",
            "('Gutenberg’s', 3)\n",
            "('Alice’s', 1)\n",
            "('in', 2)\n",
            "('Adventures', 2)\n",
            "('Wonderland', 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**groupByKey**\n",
        "\n",
        "Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.\n",
        "\n",
        "If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "znFjLNY4YHVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(rdd.groupByKey().mapValues(len).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv95vTaNYjIl",
        "outputId": "78227307-3f54-4e59-c03a-7705b6beff6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Adventures', 2),\n",
              " ('Alice’s', 1),\n",
              " ('Gutenberg’s', 3),\n",
              " ('Project', 3),\n",
              " ('Wonderland', 2),\n",
              " ('in', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(rdd.groupByKey().mapValues(list).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNcVbJNEYuVp",
        "outputId": "7f8a5c09-0846-46f8-e847-1782e25539ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Adventures', [1, 1]),\n",
              " ('Alice’s', [1]),\n",
              " ('Gutenberg’s', [1, 1, 1]),\n",
              " ('Project', [1, 1, 1]),\n",
              " ('Wonderland', [1, 1]),\n",
              " ('in', [1, 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**combineByKey**\n",
        "\n",
        "Generic function to combine the elements for each key using a custom set of aggregation functions.\n",
        "\n",
        "Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C.\n",
        "\n",
        "Users provide three functions:\n",
        "\n",
        "createCombiner, which turns a V into a C (e.g., creates a one-element list)\n",
        "\n",
        "mergeValue, to merge a V into a C (e.g., adds it to the end of a list)\n",
        "\n",
        "mergeCombiners, to combine two C’s into a single one (e.g., merges the lists)\n",
        "\n",
        "To avoid memory allocation, both mergeValue and mergeCombiners are allowed to modify and return their first argument instead of creating a new C.\n",
        "\n",
        "In addition, users can control the partitioning of the output RDD.\n",
        "\n",
        "Notes\n",
        "\n",
        "V and C can be different – for example, one might group an RDD of type\n",
        "(Int, Int) into an RDD of type (Int, List[Int]).\n",
        "\n"
      ],
      "metadata": {
        "id": "Ewa2z3uwZ4FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
        "def to_list(a):\n",
        "    return [a]\n",
        "\n",
        "def append(a, b):\n",
        "    a.append(b)\n",
        "    return a\n",
        "\n",
        "def extend(a, b):\n",
        "    a.extend(b)\n",
        "    return a\n",
        "\n",
        "sorted(x.combineByKey(to_list, append, extend).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvS3zo1NZ_5n",
        "outputId": "54788fd7-8be5-405f-f660-672407da59d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', [1, 2]), ('b', [1])]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sortByKey**\n",
        "\n",
        "Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
        "\n"
      ],
      "metadata": {
        "id": "sDrnn-Xqamap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
        "spark.sparkContext.parallelize(tmp).sortByKey().first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0pKYsfmauog",
        "outputId": "ddf27a86-f031-4b60-a135-4bed70b93b79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1', 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.parallelize(tmp).sortByKey(True, 1).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk_wU2Lka8tI",
        "outputId": "7b7fde4f-e568-4c2c-cbed-2124f5bc69cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.parallelize(tmp).sortByKey(True, 2).collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7p3dA4ka_LL",
        "outputId": "7371f326-d829-4ecf-f8c5-1f3a73225c57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
        "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
        "spark.sparkContext.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zearqZDfbEiv",
        "outputId": "50da90bb-a2bc-4951-923c-262e01dc1cab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 3),\n",
              " ('fleece', 7),\n",
              " ('had', 2),\n",
              " ('lamb', 5),\n",
              " ('little', 4),\n",
              " ('Mary', 1),\n",
              " ('was', 8),\n",
              " ('white', 9),\n",
              " ('whose', 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intersection**\n",
        "\n",
        "Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.\n",
        "\n",
        "Notes\n",
        "\n",
        "This method performs a shuffle internally."
      ],
      "metadata": {
        "id": "4_-6QFkvgkir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = spark.sparkContext.parallelize([1, 10, 2, 3, 4, 5])\n",
        "rdd2 = spark.sparkContext.parallelize([1, 6, 2, 3, 7, 8])\n",
        "rdd1.intersection(rdd2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T47eD3dIiQTd",
        "outputId": "5db8969f-151b-43b2-d8c1-e0f67afc484a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **cogroup**\n",
        "\n",
        "For each key k in self or other, return a resulting RDD that contains a tuple with the list of values for that key in self as well as other.\n",
        "\n"
      ],
      "metadata": {
        "id": "e0mhlVVsj8So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "y = spark.sparkContext.parallelize([(\"a\", 2)])\n",
        "[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M43zOWhBkBoV",
        "outputId": "d093731e-bc7b-43f0-e241-16489db5151a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', ([1], [2])), ('b', ([4], []))]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **groupWith**\n",
        "\n",
        "Alias for cogroup but with support for multiple RDDs.\n",
        "\n"
      ],
      "metadata": {
        "id": "RGh16-U-kWRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = spark.sparkContext.parallelize([(\"a\", 5), (\"b\", 6)])\n",
        "x = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "y = spark.sparkContext.parallelize([(\"a\", 2)])\n",
        "z = spark.sparkContext.parallelize([(\"b\", 42)])\n",
        "[(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C27stAfkn5a",
        "outputId": "a6dc85a2-788e-49d2-e407-acc97299684e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **distinct**\n",
        "\n",
        "Return a new RDD containing the distinct elements in this RDD.\n",
        "\n"
      ],
      "metadata": {
        "id": "OI1MR8l9k0Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(spark.sparkContext.parallelize([1, 1, 2, 3]).distinct().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ezSfsGk_77",
        "outputId": "142d6fc5-11fb-400d-cf3b-c55b69d4c169"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **repartition**\n",
        "\n",
        "Return a new RDD that has exactly numPartitions partitions.\n",
        "\n",
        "Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle.\n"
      ],
      "metadata": {
        "id": "3kWJbWE6lPxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7], 4)\n",
        "sorted(rdd.glom().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Noi2GZ9AlUWi",
        "outputId": "4c000bf8-2272-4b8f-eb9d-9923ed7bf05a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1], [2, 3], [4, 5], [6, 7]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(rdd.repartition(2).glom().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2jd2QYxlF6p",
        "outputId": "06d18ece-28e7-44fc-9d4d-bff1105284dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(rdd.repartition(10).glom().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtLGGDG2mBJU",
        "outputId": "4cc82f0a-f75e-4ac1-9c09-12f8ac64f1f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **coalesce**\n",
        "\n",
        "Return a new RDD that is reduced into numPartitions partitions."
      ],
      "metadata": {
        "id": "r0i_sMiwmUVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.parallelize([1, 2, 3, 4, 5], 3).glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR3R4lwzmdZK",
        "outputId": "7271323c-5e82-47c8-b06f-4531f01b8cb2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1], [2, 3], [4, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.parallelize([1, 2, 3, 4, 5], 3).coalesce(2).glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5M-RI3LmSzt",
        "outputId": "144c361f-0ad4-4e99-8263-c4f77d337ef6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1], [2, 3, 4, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark Join Types | Join Two DataFrames**\n",
        "\n",
        "PySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames; it supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN. PySpark Joins are wider transformations that involve data shuffling across the network.\n",
        "\n",
        "PySpark SQL Joins comes with more optimization by default (thanks to DataFrames) however still there would be some performance issues to consider while using.\n",
        "\n",
        "\n",
        "*join(self, other, on=None, how=None)*\n",
        "*join() *operation takes parameters as below and returns DataFrame.\n",
        "\n",
        "param other: Right side of the join\n",
        "param on: a string for the join column name\n",
        "param how: default inner. Must be one of *inner, cross, outer,full, full_outer, left, left_outer, right, right_outer,left_semi, and left_anti*.\n",
        "You can also write Join expression by adding where() and filter() methods on DataFrame and can have Join on multiple columns.\n",
        "\n",
        "Before we jump into PySpark SQL Join examples, first, let’s create an \"emp\" and \"dept\" DataFrames. here, column \"emp_id\" is unique on emp and \"dept_id\" is unique on the dept dataset’s and emp_dept_id from emp has a reference to dept_id on dept dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "LcLl01dsbpBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBQQ3LM1b0Qj",
        "outputId": "9d7eb852-ea16-4b77-ed59-576c74ed8ab5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- superior_emp_id: long (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inner join is the default join in PySpark and it’s mostly used. This joins two datasets on key columns, where keys don’t match the rows get dropped from both datasets (emp & dept).\n",
        "\n"
      ],
      "metadata": {
        "id": "axakzdRXckYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPGVHgIAcroH",
        "outputId": "c19d3082-6d04-4624-e8fb-99d47bbd8e24"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark Full Outer Join**\n",
        "Outer a.k.a full, fullouter join returns all rows from both datasets, where join expression doesn’t match it returns null on respective record columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "jX6hWEpbc4lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Outer Join\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYg8ogiedHzX",
        "outputId": "e643b8a1-3502-428a-89fc-c9faf2320b99"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outer Join\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full join\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLNvr9_UdWBR",
        "outputId": "b42b93f3-97f0-4e47-f7d8-7a5a0f11f2c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full join\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full outer\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjw2A0QVdcOJ",
        "outputId": "a3f42c2f-5284-4335-bc51-341909c29c1e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full outer\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark Left Outer Join**\n",
        "\n",
        "*Left* a.k.a *Leftouter* join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.\n"
      ],
      "metadata": {
        "id": "pwCFucrTeg8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Left join\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB16G-KHegfG",
        "outputId": "397da195-7772-492b-c1ac-33652ef6d53d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left join\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Leftouter join\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njOD120Pe3FV",
        "outputId": "c96245f0-2ab0-40ce-cc83-4e85e43bbba5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leftouter join\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Right Outer Join**\n",
        "\n",
        "Right a.k.a Rightouter join is opposite of left join, here it returns all rows from the right dataset regardless of math found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2PXqO4yfFvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Right join\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaQxPKTKfMBv",
        "outputId": "5140c1fa-7209-4d3c-90f9-1b14081abd01"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Right join\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Right outer join\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGWiR3aLfdUv",
        "outputId": "82b3fa01-1dce-45fa-8115-91e3ab130171"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Right outer join\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left Semi Join**\n",
        "\n",
        "*leftsemi* join is similar to inner join difference being leftsemi join returns all columns from the left dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.\n",
        "\n",
        "The same result can be achieved using select on the result of the inner join however, using this join would be efficient."
      ],
      "metadata": {
        "id": "kZ6bcgfHfwqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcHV3sjNfu-u",
        "outputId": "5c66fdf8-a2fd-428b-b943-af01035c5e16"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left Anti Join**\n",
        "\n",
        "*leftanti* join does the exact opposite of the leftsemi, leftanti join returns only columns from the left dataset for non-matched records.\n",
        "\n"
      ],
      "metadata": {
        "id": "D6lBztt8gEYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O137Db6JgIXd",
        "outputId": "f8b0ad80-fff0-4bba-be1b-c77b0624ac1d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|6     |Brown|2              |2010       |50         |      |-1    |\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self Join**\n",
        "\n",
        "Joins are not complete without a *self join*, Though there is no self-join type available, we can use any of the above-explained join types to join DataFrame to itself. below example use inner self join.\n",
        "\n"
      ],
      "metadata": {
        "id": "uXVaTmGigVKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjbuwiODgbyh",
        "outputId": "0a8aadeb-a2f5-4726-856c-afa765f30121"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------------+\n",
            "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
            "+------+--------+---------------+-----------------+\n",
            "|2     |Rose    |1              |Smith            |\n",
            "|3     |Williams|1              |Smith            |\n",
            "|4     |Jones   |2              |Rose             |\n",
            "|5     |Brown   |2              |Rose             |\n",
            "|6     |Brown   |2              |Rose             |\n",
            "+------+--------+---------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Complete join example**"
      ],
      "metadata": {
        "id": "pkrKj_wmhtrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
        "     .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
        "   .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.createOrReplaceTempView(\"EMP\")\n",
        "deptDF.createOrReplaceTempView(\"DEPT\")\n",
        "\n",
        "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnBbLGAFhyD-",
        "outputId": "e51b01ed-2a07-4dce-d2a7-c032b7aef799"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- superior_emp_id: long (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|6     |Brown|2              |2010       |50         |      |-1    |\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "\n",
            "+------+--------+---------------+-----------------+\n",
            "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
            "+------+--------+---------------+-----------------+\n",
            "|2     |Rose    |1              |Smith            |\n",
            "|3     |Williams|1              |Smith            |\n",
            "|4     |Jones   |2              |Rose             |\n",
            "|5     |Brown   |2              |Rose             |\n",
            "|6     |Brown   |2              |Rose             |\n",
            "+------+--------+---------------+-----------------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS450 Assignment 3\n",
        "\n",
        "Katya Griffiths-Julien, Jerry Lau"
      ],
      "metadata": {
        "id": "hP6M7kjROJaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Setup colab with csv file\n",
        "#import file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "5qx6dql2OPwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "7e2c30b1-57fa-47d6-fc21-70c5e54ca199"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f97db03e-0ada-40e3-b671-df80b9b64495\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f97db03e-0ada-40e3-b671-df80b9b64495\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_incomes.csv to test_incomes (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial\n",
        "\n",
        "Code to run pyspark for trial income file"
      ],
      "metadata": {
        "id": "vcz4QBQGRGOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if file can be read, count total lines\n",
        "\n",
        "with open('trial_incomes.csv', 'r') as infile:\n",
        "  i = 0#keeps track of lines read\n",
        "  for line in infile:\n",
        "    #remove \\n and convert to int\n",
        "    element = int(line.strip())\n",
        "    i += 1\n",
        "\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMUwHgPnwVMm",
        "outputId": "2e593f15-57dd-469a-c3e0-80c23a206d97"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics Tasks\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MyName\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv('trial_incomes.csv', header=False, inferSchema=True)\n",
        "\n",
        "# Print the schema of the DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Count the number of distinct values in the 'column_name' column\n",
        "distinct_count = df.distinct().count()\n",
        "medianDF = df.groupBy(df.columns[0]).count().orderBy(df.columns[0], ascending=True)\n",
        "mode = df.groupBy(df.columns[0]).count().orderBy(\"count\", ascending=False).first()[0]\n",
        "totalVal = df.count()\n",
        "\n",
        "array = np.array(medianDF.select(\"count\").collect())\n",
        "myList = np.ravel(array)\n",
        "\n",
        "mySum = 0\n",
        "median = 0\n",
        "for i in range(len(myList)):\n",
        "  mySum += myList[i]\n",
        "  if mySum >= totalVal // 2:\n",
        "    median = i+1\n",
        "    break\n",
        "\n",
        "print(f\"The number of distinct values in the csv file is {distinct_count}.\")\n",
        "print(f\"The median of the csv file is {median}.\")\n",
        "print(f\"The mode of the csv file is {mode}.\")\n",
        "\n",
        "#df.select(median(df.columns[0])).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOdhFeGqw1lZ",
        "outputId": "9183b04f-9d70-4d8b-cf66-96e2f656e31a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            "\n",
            "The number of distinct values in the csv file is 79.\n",
            "The median of the csv file is 4.\n",
            "The mode of the csv file is 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count per 10 power\n",
        "\n",
        "from pyspark.sql.functions import log10, floor\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MyNewName\").getOrCreate()\n",
        "\n",
        "# Create a PySpark DataFrame with integer values\n",
        "df = spark.read.csv('trial_incomes.csv', header=False, inferSchema=True)\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "# Round each integer down to its nearest power of 10\n",
        "df = df.withColumn(\"rounded_value\", floor(log10(df.columns[0])))\n",
        "\n",
        "# Group the DataFrame by the rounded values and count the number of occurrences of each value\n",
        "counts = df.groupBy(\"rounded_value\").count()\n",
        "\n",
        "# Print the counts to the console\n",
        "print(counts.orderBy(\"rounded_value\").show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeDLEf3u6696",
        "outputId": "0d2f22b9-e956-4b12-cbdc-3a8cd78b641c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            "\n",
            "+-------------+-----+\n",
            "|rounded_value|count|\n",
            "+-------------+-----+\n",
            "|            0|  853|\n",
            "|            1|  118|\n",
            "|            2|   19|\n",
            "|            3|    8|\n",
            "|            4|    2|\n",
            "+-------------+-----+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "\n",
        "Code to run pyspark for test income file"
      ],
      "metadata": {
        "id": "Ts5UTEn0RJVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if file can be read, count total lines\n",
        "\n",
        "with open('test_incomes.csv', 'r') as infile:\n",
        "  i = 0#keeps track of lines read\n",
        "  for line in infile:\n",
        "    #remove \\n and convert to int\n",
        "    element = int(line.strip())\n",
        "    i += 1\n",
        "\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "0T4iyQjyRLrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fa4100-73fe-4919-d653-64ee242a8c60"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics Tasks for test\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MyNameTest\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv('test_incomes.csv', header=False, inferSchema=True)\n",
        "\n",
        "# Print the schema of the DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Count the number of distinct values in the 'column_name' column\n",
        "distinct_count = df.distinct().count()\n",
        "medianDF = df.groupBy(df.columns[0]).count().orderBy(df.columns[0], ascending=True)\n",
        "mode = df.groupBy(df.columns[0]).count().orderBy(\"count\", ascending=False).first()[0]\n",
        "totalVal = df.count()\n",
        "\n",
        "array = np.array(medianDF.select(\"count\").collect())\n",
        "myList = np.ravel(array)\n",
        "\n",
        "mySum = 0\n",
        "median = 0\n",
        "for i in range(len(myList)):\n",
        "  mySum += myList[i]\n",
        "  if mySum >= totalVal // 2:\n",
        "    median = i+1\n",
        "    break\n",
        "\n",
        "print(f\"The number of distinct values in the csv file is {distinct_count}.\")\n",
        "print(f\"The median of the csv file is {median}.\")\n",
        "print(f\"The mode of the csv file is {mode}.\")\n",
        "\n",
        "#df.select(median(col(df.columns[0]))).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FKqQKBVU-Oa",
        "outputId": "423c79f7-1deb-496d-811c-de2dab7472e4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: long (nullable = true)\n",
            "\n",
            "The number of distinct values in the csv file is 20872.\n",
            "The median of the csv file is 3.\n",
            "The mode of the csv file is 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count per 10 power for test\n",
        "\n",
        "from pyspark.sql.functions import log10, floor\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MyNewNameTest\").getOrCreate()\n",
        "\n",
        "# Create a PySpark DataFrame with integer values\n",
        "df = spark.read.csv('test_incomes.csv', header=False, inferSchema=True)\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "# Round each integer down to its nearest power of 10\n",
        "df = df.withColumn(\"rounded_value\", floor(log10(df.columns[0])))\n",
        "\n",
        "# Group the DataFrame by the rounded values and count the number of occurrences of each value\n",
        "counts = df.groupBy(\"rounded_value\").count()\n",
        "\n",
        "# Print the counts to the console\n",
        "print(counts.orderBy(\"rounded_value\").show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iogTB8-YVJD3",
        "outputId": "3add8c7d-6adf-4a25-fc10-4b99f606a334"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: long (nullable = true)\n",
            "\n",
            "+-------------+-------+\n",
            "|rounded_value|  count|\n",
            "+-------------+-------+\n",
            "|            0|8245447|\n",
            "|            1|1414970|\n",
            "|            2| 271544|\n",
            "|            3|  54505|\n",
            "|            4|  10876|\n",
            "|            5|   2137|\n",
            "|            6|    415|\n",
            "|            7|     89|\n",
            "|            8|     13|\n",
            "|            9|      2|\n",
            "|           10|      1|\n",
            "|           11|      1|\n",
            "+-------------+-------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other\n",
        "\n",
        "Code to run numpy test"
      ],
      "metadata": {
        "id": "weJnfLjBXzce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple numpy test for execution time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = np.genfromtxt('trial_incomes.csv', delimiter=',')\n",
        "\n",
        "distinct_count = len(np.unique(data))\n",
        "vals, counts = np.unique(data, return_counts=True)\n",
        "\n",
        "#find mode\n",
        "mode_value = np.argwhere(counts == np.max(counts))\n",
        "mode = vals[mode_value].flatten().tolist()\n",
        "\n",
        "median = np.median(data)\n",
        "\n",
        "print(f\"The number of distinct values in the csv file is {distinct_count}.\")\n",
        "print(f\"The median of the csv file is {median}.\")\n",
        "print(f\"The mode of the csv file is {mode}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VImstvh1ucU",
        "outputId": "b44f91b3-6420-44c5-ba91-09013f85a583"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of distinct values in the csv file is 79.\n",
            "The median of the csv file is 4.0.\n",
            "The mode of the csv file is 4.0.\n"
          ]
        }
      ]
    }
  ]
}